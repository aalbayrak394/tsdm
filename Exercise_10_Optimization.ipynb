{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import data_shift\n",
    "import hiplot as hip\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from copy import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this exercise, you need to install the library firstly:\n",
    "\n",
    "- hiplot: conda install -c conda-forge hiplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Neural Network Implementation and Optimization\n",
    "\n",
    "In this exercise you need to implement a fully-connected neural network using PyTorch to forecast daily minimum temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Import temperature time-series dataset **daily-minimum-temperatures.csv** in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/daily-minimum-temperatures.csv',header=0, index_col=0)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Generate a **7**-lag input dataset manually, i.e., use the previous 7 days data \\[x(t-7), x(t-6)...x(t-1)\\] as the input to predict the following day X(t) using the given function **data_shift()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 7\n",
    "train, test = data[0:-50], data[-50:]\n",
    "\n",
    "# Scaling the train/test data using minmaxscaler\n",
    "### Your Code Here ###       \n",
    "\n",
    "print(train_scaled.head(5))\n",
    "print(test_scaled.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the shifted training/test dataset, using the given function data_shift()\n",
    "### Your Code Here ### \n",
    "\n",
    "# Split the train/test dataset to train_X, train_y / test_X, test_y\n",
    "### Your Code Here ### \n",
    "\n",
    "# Convert the train/test dataset to tensors, the data type is torch.float32\n",
    "### Your Code Here ### \n",
    "\n",
    "print(train_X_tensor[:3])\n",
    "print(train_y_tensor[:3])\n",
    "print(test_X_tensor[:3])\n",
    "print(test_y_tensor[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**  Complete the given class **dnn()** using PyTorch to implement a neural network.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- [**nn.Sequential()**](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dnn(nn.Module):\n",
    "    def __init__(self, input_size=7, network_structure=[16, 4], output_size=1, activation_function='sigmoid', \n",
    "                 dropout_rate=0.2, leaky_relu_slop=0.01, output_activation_function=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        input_size: int, the dimension of the input features.\n",
    "        network_structure: list, the strcture of the hidden layers in the network, each number corresponds to the number \n",
    "                           of neuros in a hidden layer.\n",
    "        output_size: int, the dimension of the prediction target.\n",
    "        activation_function: string, the name of the selected activation function in hidden layers. Implement the function\n",
    "                             str2act() to convert the string to activation function.\n",
    "        dropout_rate: float, the probability of an element to be zeroed.\n",
    "        leaky_relu_slop: float, controls the angle of the negative slope for the function leakyrelu.\n",
    "        output_activation_function: string, the name of the selected activation function in output layer.       \n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.leaky_relu_slop = leaky_relu_slop\n",
    "        self.activation_function = self.str2act(activation_function)\n",
    "        self.output_activation_function = self.str2act(output_activation_function)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.network_structure = copy(network_structure)\n",
    "        self.network_structure.insert(0, self.input_size)\n",
    "        \n",
    "        # Implement the neural network model using pytorch modules based on the given input/output size \n",
    "        # and the network structure.\n",
    "        \n",
    "        ### Your Code Here ### \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: torch.tensor, the input\n",
    "        \n",
    "        Feedforward propagation of the input to the output. \n",
    "        '''\n",
    "        ### Your Code Here ### \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        x: troch.tensor, the input\n",
    "        \n",
    "        Make a prediction for the input. The mode should be set to evaluation mode there.\n",
    "        '''\n",
    "        training_mode = self.training # store the current mode\n",
    "        self.eval() # to evaluation mode\n",
    "        \n",
    "        # predict\n",
    "        ### Your Code Here ### \n",
    "        \n",
    "        self.train(training_mode) # reset the saved mode\n",
    "        return output\n",
    "    \n",
    "    def str2act(self, str):\n",
    "        '''\n",
    "        Convert the given string to the corresponding activation function.\n",
    "        '''\n",
    "        \n",
    "        if str == \"sigmoid\":\n",
    "            \n",
    "            pass\n",
    "            ### Your Code Here ### \n",
    "        \n",
    "        elif str == \"tanh\":\n",
    "            \n",
    "            pass\n",
    "            ### Your Code Here ###     \n",
    "        \n",
    "        elif str == \"relu\":\n",
    "            \n",
    "            pass\n",
    "            ### Your Code Here ### \n",
    "            \n",
    "        \n",
    "        elif str == \"leakyrelu\":\n",
    "            \n",
    "            pass\n",
    "            ### Your Code Here ### \n",
    "            \n",
    "        elif str == None:\n",
    "            # No activate function is required.\n",
    "            # the function should return the input\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dnn()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Train the **model** using mini-batch gradient descent.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Convert the training tensors to iterable objects using the data loading utility [**DataLoader()**](https://pytorch.org/docs/stable/data.html). \n",
    "\n",
    "- Use [MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) as the loss function\n",
    "\n",
    "- Use SGD with a optimial learning rate as the [optimizer](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 256\n",
    "batch_size = 32\n",
    "input_size = train_X_tensor.shape[1]\n",
    "output_size = 1\n",
    "network_structure = [64, 32, 16, 1]\n",
    "activation_function = 'leakyrelu'\n",
    "leaky_relu_slop = 0.01\n",
    "dropout_rate = 0\n",
    "output_activation_function = None\n",
    "\n",
    "# Convert the training tensors to iterable objects\n",
    "### Your Code Here ### \n",
    "\n",
    "# Initialize the model\n",
    "### Your Code Here ### \n",
    "\n",
    "# Define loss function\n",
    "### Your Code Here ### \n",
    "\n",
    "# Define optimizer\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [] # save the average training loss of each iteration\n",
    "test_losses = [] # save the average test loss of each iteration\n",
    "\n",
    "# Training process\n",
    "for epoch in range(epochs):\n",
    "    # set training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Implement the training loop using mini-batch \n",
    "    for idx, batch in enumerate(loader):\n",
    "        pass\n",
    "        ### Your Code Here ### \n",
    "        \n",
    "    # compute test error in each epoch and add it to test_losses\n",
    "    # Do the same for training loss as well\n",
    "    # print the test error in each 10 epochs\n",
    "    ### Your Code Here ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the training data\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What problem you can observe in this figure? What could cause this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Implement [**Early Stopping**](https://en.wikipedia.org/wiki/Early_stopping) in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-initialize the model\n",
    "### Your Code Here ### \n",
    "\n",
    "# The training process is monitored and stopped when the loss doesn't decrease within 30 iterations.\n",
    "early_stop_patience = 30\n",
    "\n",
    "train_losses = [] # save the average training loss of each iteration\n",
    "test_losses = [] # save the average test loss of each iteration\n",
    "best_epoch = 0 # the epoch of the lowest test loss\n",
    "lowest_loss = sys.float_info.max # the current lowest test loss\n",
    "epochs_counter = 0 # count how many epochs have been monitored and compare the number to the patience.\n",
    "\n",
    "# Define loss function\n",
    "### Your Code Here ### \n",
    "\n",
    "# Define optimizer SGD\n",
    "### Your Code Here ### \n",
    "\n",
    "# Define optimizer Adam (for subtask f)\n",
    "### Your Code Here ### \n",
    "\n",
    "# Training process\n",
    "for epoch in range(epochs):\n",
    "    # set training mode\n",
    "    model.train()\n",
    "    running_losses = 0.0\n",
    "    \n",
    "     # Implement the training loop using mini-batch\n",
    "    for idx, batch in enumerate(loader):\n",
    "        pass\n",
    "        ### Your Code Here ### \n",
    "    \n",
    "    # compute test error in each epoch and add it to test_losses\n",
    "    # Do the same for training loss as well\n",
    "    # print the test error in each 10 epochs\n",
    "    \n",
    "    ### Your Code Here ###\n",
    "    \n",
    "    # Implment early stopping\n",
    "    # Update the lowest loss, when the newest test loss is smaller.\n",
    "    # Otherwise stop_epochs += 1\n",
    "    # Stop learning and return the lowest test loss, when the stop_epochs is greater than or equal to\n",
    "    # the early_stop_patience\n",
    "    \n",
    "    ### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data\n",
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Try [**Adam**](https://pytorch.org/docs/stable/optim.html) optimzer based on the code of sub-task **e** and plot the learning curve. What different can you observe between the learning curve of SGD and Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** Search the optimal hyperarameters using **grid search** and visualize the searching results with [HiPlot](https://github.com/facebookresearch/hiplot).\n",
    "\n",
    "- Optimize these hyperparameters: **the structure of neural networks** (i.e., the number of neuros and layers), **activation functions**, **dropout rates**, **optimizers**, **learning rate**.\n",
    "\n",
    "- The searching range can be adjusted depending on your hardware condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_structures = [[32, 16, 1], [64, 32, 16, 1]]\n",
    "activation_functions = ['tanh', 'sigmoid']\n",
    "dropout_rates = [0.2, 0.05]\n",
    "learning_rates = [0.05, 0.01]\n",
    "optimizers = ['adam', 'sgd']\n",
    "results = []\n",
    "param_idx = 1\n",
    "\n",
    "# The training process is monitored and stopped when the loss doesn't decrease within 30 iterations.\n",
    "early_stop_patience = 30\n",
    "\n",
    "# Search the optimal parameters using grid search\n",
    "for nn_structure in nn_structures:\n",
    "    for activation_function in activation_functions:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            for learning_rate in learning_rates:\n",
    "                for optimizer_str in optimizers:\n",
    "                    \n",
    "                    # Training settings\n",
    "                    # declare variables, e.g. losses list, lowest loss etc\n",
    "                    # initialize the neural network, optimizer, and the corresponding loss function.\n",
    "                    ### Your Code Here ###\n",
    "                    \n",
    "                    print(f\"The index of the hyparameter set: {param_idx}\")\n",
    "                    \n",
    "                    # Implement the training loop using mini-batch\n",
    "                    for epoch in range(epochs):\n",
    "                        # set training mode\n",
    "                        model.train()\n",
    "                        running_losses = 0.0\n",
    "                        \n",
    "                        # Implement the training loop using mini-batch\n",
    "                        for idx, batch in enumerate(loader):\n",
    "                            pass\n",
    "                            ### Your Code Here ###\n",
    "                        \n",
    "                        # compute test error in each epoch and add it to test_losses\n",
    "                        # Do the same for training loss as well\n",
    "                        # print the test error in each 10 epochs\n",
    "                        \n",
    "                        ### Your Code Here ###\n",
    "                        \n",
    "                        # Implment early stopping\n",
    "                        # Update the lowest loss, when the newest test loss is smaller.\n",
    "                        # Otherwise stop_epochs += 1\n",
    "                        # Stop learning and return the lowest test loss, when the stop_epochs is greater than or equal to\n",
    "                        # the early_stop_patience\n",
    "                        \n",
    "                        ### Your Code Here ###\n",
    "                        \n",
    "                    # Store the parameters and the result for displaying    \n",
    "                    results.append({\"Structure\": nn_structure,\n",
    "                                    \"activation function\": activation_function,\n",
    "                                    \"dropout rate\": dropout_rate,\n",
    "                                    \"learning rate\": learning_rate,\n",
    "                                    \"optimizer\": optimizer_str,\n",
    "                                    \"test error\": lowest_loss\n",
    "                                    })\n",
    "                    param_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results in parallel plots.\n",
    "hip.Experiment.from_iterable(results).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
